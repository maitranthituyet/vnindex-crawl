name: Scrape VNIndex Data

on:
  schedule:
    - cron: "10 8 * * 1-5"  # Runs at 08:10 UTC (15:10 Vietnam Time, GMT+7)
  workflow_dispatch:  # Allows manual runs

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas gspread oauth2client selenium

      - name: Install Chrome & Chromedriver
        run: |
          sudo apt update
          sudo apt install -y chromium-browser

          # Get Chrome major version
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d '.' -f 1)
          
          # Fetch the latest compatible Chromedriver version
          DRIVER_VERSION=$(curl -sS "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
          
          # Download and install Chromedriver
          wget -N "https://chromedriver.storage.googleapis.com/$DRIVER_VERSION/chromedriver_linux64.zip"
          unzip -o chromedriver_linux64.zip
          sudo mv chromedriver /usr/bin/chromedriver
          sudo chmod +x /usr/bin/chromedriver

      - name: Verify Chrome & Chromedriver Versions
        run: |
          google-chrome --version
          chromedriver --version

      - name: Run Scraper
        env:
          GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        run: python vnindex_crawl.py
